"""
æŠ–éŸ³æ”¶è—æµ·æŠ¥å¢™ - å…¨è‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
ä¸€é”®è¿è¡Œï¼šæ‰«ç ç™»å½• â†’ è‡ªåŠ¨é‡‡é›† â†’ è‡ªåŠ¨ä¸‹è½½ â†’ è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
"""
import os
import sys
import json
import asyncio
import webbrowser
import subprocess
from pathlib import Path
from http.server import HTTPServer, SimpleHTTPRequestHandler
from http.server import HTTPServer, SimpleHTTPRequestHandler
from threading import Thread
import time
import re


# é¡¹ç›®è·¯å¾„
PROJECT_DIR = Path(__file__).parent.resolve()
DATA_DIR = PROJECT_DIR / "data"
COVERS_DIR = DATA_DIR / "covers"
METADATA_PATH = DATA_DIR / "metadata.json"

# é…ç½®
MAX_ITEMS = 2000  # æœ€å¤§é‡‡é›†æ•°é‡
CONCURRENCY = 10  # å¹¶å‘ä¸‹è½½æ•°
SERVER_PORT = 5000


def check_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–"""
    try:
        from playwright.async_api import async_playwright
        import aiohttp
        import aiofiles
        import pyperclip
        import pygetwindow as gw
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
        return True
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("ğŸ”§ æ­£åœ¨å®‰è£…ä¾èµ–...")
        subprocess.run([sys.executable, "-m", "pip", "install", 
                       "playwright", "aiohttp", "aiofiles", "pyperclip", "pygetwindow", "-q"])
        subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"])
        return True


async def login_and_scrape_favorites():
    """
    ä½¿ç”¨ Playwright ç™»å½•å¹¶ç›´æ¥ä»é¡µé¢æŠ“å–æ”¶è—å¤¹æ•°æ®
    """
    from playwright.async_api import async_playwright
    
    print("\n" + "="*50)
    print("ğŸ“± è¯·ä½¿ç”¨æŠ–éŸ³ APP æ‰«æäºŒç»´ç ç™»å½•")
    print("="*50)
    print("\nâš ï¸  ç™»å½•åè¯·å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š")
    print("   1. å¤„ç†æ‰€æœ‰å¼¹çª—ï¼ˆå¦‚'ä¿å­˜ç™»å½•ä¿¡æ¯'ã€'èº«ä»½éªŒè¯'ç­‰ï¼‰")
    print("   2. ç¡®ä¿èƒ½çœ‹åˆ°ä½ çš„æ”¶è—å¤¹é¡µé¢")
    print("   3. å›åˆ°å‘½ä»¤è¡ŒæŒ‰ Enter é”®ç»§ç»­\n")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        
        # æ‰“å¼€æŠ–éŸ³æ”¶è—é¡µé¢
        await page.goto("https://www.douyin.com/user/self?showTab=favorite_collection")
        
        # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤ç™»å½•
        print("â³ ç­‰å¾…ç™»å½•...")
        
        import threading
        user_confirmed = threading.Event()
        
        def wait_for_input():
            input("âœ… ç™»å½•å®Œæˆåï¼ŒæŒ‰ Enter é”®ç»§ç»­...")
            user_confirmed.set()
        
        input_thread = threading.Thread(target=wait_for_input, daemon=True)
        input_thread.start()
        
        while not user_confirmed.is_set():
            await asyncio.sleep(0.5)
            if page.is_closed():
                print("âŒ æµè§ˆå™¨å·²å…³é—­")
                return []
        
        print("\nğŸ”„ æ­£åœ¨ä»é¡µé¢æŠ“å–æ”¶è—å¤¹æ•°æ®...")
        print("   ï¼ˆè¯·ä¿æŒæµè§ˆå™¨çª—å£æ‰“å¼€ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨æ»šåŠ¨åŠ è½½æ›´å¤šï¼‰\n")
        
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
        await asyncio.sleep(3)
        
        all_videos = []
        last_count = 0
        no_new_count = 0
        scroll_count = 0
        
        while len(all_videos) < MAX_ITEMS:
            try:
                # ä½¿ç”¨æ›´å®½æ³›çš„é€‰æ‹©å™¨ä»é¡µé¢è·å–è§†é¢‘æ•°æ®
                videos_data = await page.evaluate('''() => {
                    const videos = [];
                    const seen = new Set();
                    
                    // ç­–ç•¥1: æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰è§†é¢‘é“¾æ¥çš„ a æ ‡ç­¾
                    document.querySelectorAll('a[href*="/video/"]').forEach(link => {
                        const videoId = link.href.match(/\\/video\\/([\\d]+)/)?.[1];
                        if (!videoId || seen.has(videoId)) return;
                        seen.add(videoId);
                        
                        // åœ¨é“¾æ¥å†…æˆ–é™„è¿‘æ‰¾å°é¢å›¾
                        const container = link.closest('li, div[class], article') || link;
                        const img = container.querySelector('img[src*="douyinpic"], img[src*="bytedance"], img[src*="tiktokcdn"]') 
                                 || container.querySelector('img')
                                 || link.querySelector('img');
                        
                        if (img && img.src && !img.src.includes('avatar')) {
                            videos.push({
                                id: videoId,
                                cover_url: img.src,
                                title: img.alt || container.textContent?.slice(0, 50) || 'æ— æ ‡é¢˜',
                                video_url: link.href
                            });
                        }
                    });
                    
                    // ç­–ç•¥2: å¦‚æœç­–ç•¥1æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ‰¾æ‰€æœ‰å¯èƒ½æ˜¯å°é¢çš„å›¾ç‰‡
                    if (videos.length === 0) {
                        document.querySelectorAll('img').forEach((img, idx) => {
                            // åªè¦æ˜¯æŠ–éŸ³CDNçš„å›¾ç‰‡ä¸”å°ºå¯¸åˆç†
                            if (img.src && 
                                (img.src.includes('douyinpic') || img.src.includes('bytedance') || img.src.includes('tiktokcdn')) &&
                                !img.src.includes('avatar') &&
                                img.width > 50 && img.height > 50) {
                                
                                const container = img.closest('a, li, div[class]');
                                const link = container?.querySelector('a[href*="/video/"]') || container?.closest('a[href*="/video/"]');
                                const videoId = link?.href?.match(/\\/video\\/([\\d]+)/)?.[1] || `img_${idx}`;
                                
                                if (!seen.has(videoId)) {
                                    seen.add(videoId);
                                    videos.push({
                                        id: videoId,
                                        cover_url: img.src,
                                        title: img.alt || 'æ— æ ‡é¢˜',
                                        video_url: link?.href || ''
                                    });
                                }
                            }
                        });
                    }
                    
                    return videos;
                }''')
                
                if videos_data and len(videos_data) > 0:
                    # å»é‡æ·»åŠ 
                    existing_ids = {v.get('id') for v in all_videos}
                    new_count = 0
                    for video in videos_data:
                        if video.get('id') and video.get('id') not in existing_ids:
                            all_videos.append(video)
                            existing_ids.add(video.get('id'))
                            new_count += 1
                    
                    if new_count > 0:
                        no_new_count = 0
                
                scroll_count += 1
                print(f"\r   ğŸ“¥ å·²è·å– {len(all_videos)} ä¸ªè§†é¢‘ (æ»šåŠ¨ {scroll_count} æ¬¡)...", end="", flush=True)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
                if len(all_videos) == last_count:
                    no_new_count += 1
                    # éœ€è¦æ›´å¤šæ¬¡æ— æ–°æ•°æ®æ‰åœæ­¢ï¼ˆç»™é¡µé¢æ›´å¤šåŠ è½½æ—¶é—´ï¼‰
                    if no_new_count >= 15:
                        print()  # æ¢è¡Œ
                        debug_info = await page.evaluate('''() => {
                            return {
                                allImages: document.querySelectorAll('img').length,
                                douyinImages: document.querySelectorAll('img[src*="douyinpic"], img[src*="bytedance"]').length,
                                videoLinks: document.querySelectorAll('a[href*="/video/"]').length,
                                url: window.location.href,
                                scrollHeight: document.body.scrollHeight,
                                noMore: document.body.innerText.includes('æ²¡æœ‰æ›´å¤š') || document.body.innerText.includes('åˆ°åº•äº†')
                            };
                        }''')
                        print(f"   ğŸ” è°ƒè¯•: {debug_info['videoLinks']} ä¸ªè§†é¢‘é“¾æ¥, é¡µé¢é«˜åº¦ {debug_info['scrollHeight']}px")
                        if debug_info.get('noMore'):
                            print("   ğŸ“‹ æ£€æµ‹åˆ°'æ²¡æœ‰æ›´å¤š'æç¤ºï¼Œå·²åŠ è½½æ‰€æœ‰æ”¶è—")
                        else:
                            print("   ğŸ“‹ è¿ç»­15æ¬¡æ— æ–°æ•°æ®ï¼Œåœæ­¢æ»šåŠ¨")
                        break
                else:
                    no_new_count = 0
                    last_count = len(all_videos)
                
                # ä½¿ç”¨é¼ æ ‡æ»šè½®æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æ»šåŠ¨ï¼ˆè§¦å‘è™šæ‹Ÿæ»šåŠ¨åŠ è½½ï¼‰
                # å…ˆå°†é¼ æ ‡ç§»åˆ°é¡µé¢ä¸­å¤®
                await page.mouse.move(500, 400)
                # æ¨¡æ‹Ÿå¤šæ¬¡æ»šè½®æ»šåŠ¨
                for _ in range(5):
                    await page.mouse.wheel(0, 800)  # å‚ç›´æ»šåŠ¨ 800 åƒç´ 
                    await asyncio.sleep(0.3)
                
                await asyncio.sleep(1.5)  # ç»™é¡µé¢æ—¶é—´åŠ è½½æ–°å†…å®¹
                
            except Exception as e:
                print(f"   âš ï¸ æŠ“å–å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                break
        
        await browser.close()
        print(f"âœ… å…±è·å– {len(all_videos)} ä¸ªæ”¶è—è§†é¢‘")
        return all_videos


def extract_cover_data(videos: list) -> list:
    """æå–å°é¢å…ƒæ•°æ®"""
    covers = []
    
    for video in videos:
        try:
            # è·å–å°é¢URL
            cover_url = ""
            if video.get("video", {}).get("cover", {}).get("url_list"):
                cover_url = video["video"]["cover"]["url_list"][0]
            elif video.get("video", {}).get("origin_cover", {}).get("url_list"):
                cover_url = video["video"]["origin_cover"]["url_list"][0]
            
            if not cover_url:
                continue
            
            cover_info = {
                "id": video.get("aweme_id", ""),
                "title": video.get("desc", "æ— æ ‡é¢˜")[:100],  # é™åˆ¶é•¿åº¦
                "author": video.get("author", {}).get("nickname", "æœªçŸ¥"),
                "author_id": video.get("author", {}).get("sec_uid", ""),
                "cover_url": cover_url,
                "video_url": f"https://www.douyin.com/video/{video.get('aweme_id', '')}",
                "create_time": video.get("create_time", 0),
            }
            covers.append(cover_info)
            
        except Exception as e:
            continue
    
    return covers


async def download_covers(covers: list):
    """å¹¶å‘ä¸‹è½½å°é¢å›¾ç‰‡"""
    import aiohttp
    import aiofiles
    
    print(f"\nğŸ“· å¼€å§‹ä¸‹è½½ {len(covers)} å¼ å°é¢...")
    
    COVERS_DIR.mkdir(parents=True, exist_ok=True)
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    downloaded = 0
    
    async def download_one(session, cover):
        nonlocal downloaded
        async with semaphore:
            url = cover.get("cover_url", "")
            if not url:
                return
            
            video_id = cover.get("id", "unknown")
            save_path = COVERS_DIR / f"{video_id}.jpg"
            
            if save_path.exists():
                cover["local_cover"] = f"data/covers/{video_id}.jpg"
                downloaded += 1
                return
            
            try:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                    if resp.status == 200:
                        content = await resp.read()
                        async with aiofiles.open(save_path, "wb") as f:
                            await f.write(content)
                        cover["local_cover"] = f"data/covers/{video_id}.jpg"
                        downloaded += 1
            except:
                pass
    
    connector = aiohttp.TCPConnector(limit=CONCURRENCY)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [download_one(session, cover) for cover in covers]
        await asyncio.gather(*tasks)
    
    print(f"âœ… ä¸‹è½½å®Œæˆ: {downloaded}/{len(covers)}")
    return covers


def save_metadata(covers: list):
    """ä¿å­˜å…ƒæ•°æ®"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(covers, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ å…ƒæ•°æ®å·²ä¿å­˜: {METADATA_PATH}")


def start_server_and_open_browser():
    """å¯åŠ¨æœåŠ¡å™¨å¹¶æ‰“å¼€æµè§ˆå™¨"""
    os.chdir(PROJECT_DIR)
    
    class ProxyHandler(SimpleHTTPRequestHandler):
        def log_message(self, format, *args):
            pass  # é™é»˜è¾“å‡º

        def do_GET(self):
            # API: è§£æè§†é¢‘ä¿¡æ¯ /api/resolve_video?url=...
            if self.path.startswith('/api/resolve_video'):
                try:
                    from urllib.parse import urlparse, parse_qs, unquote
                    import urllib.request
                    import urllib.error
                    import re
                    import json
                    
                    query = parse_qs(urlparse(self.path).query)
                    share_url = query.get('url', [None])[0]
                    
                    if not share_url:
                        self.send_error(400, "Missing url parameter")
                        return

                    # 1. è·å–HTML (æ¨¡æ‹Ÿæ‰‹æœºUAä»¥è·å–ç®€å•ç»“æ„)
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                    }
                    
                    req = urllib.request.Request(share_url, headers=headers)
                    html = ""
                    final_url = ""
                    
                    # è‡ªåŠ¨å¤„ç†é‡å®šå‘
                    with urllib.request.urlopen(req) as response:
                        html = response.read().decode('utf-8', errors='ignore')
                        final_url = response.geturl()
                    
                    # 2. æå–ä¿¡æ¯
                    result = {
                        'id': '',
                        'title': 'æœªå‘½åè§†é¢‘',
                        'author': 'æœªçŸ¥ä½œè€…',
                        'cover_url': '',
                        'video_url': final_url, # ç½‘é¡µé“¾æ¥
                        'real_video_url': ''    # MP4é“¾æ¥
                    }
                    
                    # å°è¯•ä» URL æå– ID
                    id_match = re.search(r'/video/(\d+)', final_url)
                    if id_match:
                        result['id'] = id_match.group(1)
                    else:
                        result['id'] = f"import_{int(time.time())}"
                        
                    # æå–æ ‡é¢˜ (title æ ‡ç­¾é€šå¸¸åŒ…å«)
                    title_match = re.search(r'<title>(.*?)</title>', html)
                    if title_match:
                        title_text = title_match.group(1)
                        # å»é™¤åç¼€
                        result['title'] = re.sub(r' - æŠ–éŸ³.*', '', title_text).strip()
                        
                    # æå–çœŸå®è§†é¢‘åœ°å€ (JSON æˆ– src å±æ€§)
                    # ç­–ç•¥1: æŸ¥æ‰¾ RENDER_DATA
                    # ç­–ç•¥2: æ­£åˆ™æŸ¥æ‰¾ src
                    
                    # æŸ¥æ‰¾åŒ…å« play_addr æˆ– src çš„ URLï¼Œé€šå¸¸æ˜¯ v26 æˆ– aweme åŸŸå
                    # è¿™é‡Œçš„æ­£åˆ™éœ€è¦å®½æ³›ä¸€äº›
                    # å¯»æ‰¾ "src":"https:..." ç»“æ„
                    src_matches = re.findall(r'"src":"(https?://[^"]+?)"', html)
                    for src in src_matches:
                        src = src.replace(r'\u0026', '&')
                        if ('/video/' in src or 'aweme' in src) and '.mp3' not in src and 'avatar' not in src:
                             result['real_video_url'] = src
                             break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ‰¾ playAddr
                    if not result['real_video_url']:
                        play_addr_matches = re.findall(r'"playAddr":\[{"src":"(https?://[^"]+?)"', html)
                        for src in play_addr_matches:
                             src = src.replace(r'\u0026', '&')
                             result['real_video_url'] = src
                             break

                    # æå–å°é¢
                    cover_matches = re.findall(r'"cover":"(https?://[^"]+?)"', html)
                    if cover_matches:
                        result['cover_url'] = cover_matches[0].replace(r'\u0026', '&')

                    # 3. è¿”å› JSON
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.send_header('Access-Control-Allow-Origin', '*')
                    self.end_headers()
                    self.wfile.write(json.dumps(result).encode('utf-8'))
                    
                except Exception as e:
                    self.send_error(500, str(e))
                return

            # è§†é¢‘ä»£ç†æ¥å£ï¼š/proxy_video?url=...
            if self.path.startswith('/proxy_video'):
                try:
                    from urllib.parse import urlparse, parse_qs
                    import urllib.request
                    
                    query = parse_qs(urlparse(self.path).query)
                    video_url = query.get('url', [None])[0]
                    
                    if not video_url:
                        self.send_error(400, "Missing url parameter")
                        return

                    # è½¬å‘è¯·æ±‚ï¼Œæ”¯æŒ Range è¯·æ±‚
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Referer': 'https://www.douyin.com/',
                        'Accept': '*/*',
                        'Connection': 'keep-alive',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    }
                    
                    # é€ä¼  Range headerï¼ˆHTML5 video éœ€è¦ï¼‰
                    range_header = self.headers.get('Range')
                    if range_header:
                        headers['Range'] = range_header
                    
                    # print(f"DEBUG: Proxying video: {video_url[:100]}... Range: {range_header}")
                    
                    req = urllib.request.Request(video_url, headers=headers)
                    
                    try:
                        with urllib.request.urlopen(req, timeout=10) as response:
                            # æ ¹æ®æ˜¯å¦æœ‰ Range è¿”å›ä¸åŒçŠ¶æ€ç 
                            if range_header and response.status == 206:
                                self.send_response(206)
                                content_range = response.headers.get('Content-Range')
                                if content_range:
                                    self.send_header('Content-Range', content_range)
                            else:
                                self.send_response(200)
                            
                            # é€ä¼ å…³é”®å“åº”å¤´
                            self.send_header('Content-Type', response.headers.get('Content-Type', 'video/mp4'))
                            content_length = response.headers.get('Content-Length')
                            if content_length:
                                self.send_header('Content-Length', content_length)
                            self.send_header('Accept-Ranges', 'bytes')
                            self.send_header('Access-Control-Allow-Origin', '*')
                            self.end_headers()
                            
                            # æµå¼ä¼ è¾“
                            while True:
                                chunk = response.read(65536)  # 64KB chunks
                                if not chunk: break
                                try:
                                    self.wfile.write(chunk)
                                except (ConnectionResetError, BrokenPipeError):
                                    break
                    except urllib.error.URLError as e:
                        print(f"âŒ Proxy URL Error: {e.reason} for {video_url[:100]}")
                        self.send_error(502, f"Target URL error: {e.reason}")
                    except Exception as e:
                        print(f"âŒ Proxy request failed: {str(e)}")
                        self.send_error(500, str(e))
                                
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                return

            # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶è¯·æ±‚ï¼Œæ­£å¸¸å¤„ç†
            super().do_GET()
        
        def do_POST(self):
            # API: ä¿å­˜æ•°æ®åˆ° metadata.json
            if self.path == '/api/save_data':
                try:
                    import json
                    content_length = int(self.headers.get('Content-Length', 0))
                    post_data = self.rfile.read(content_length)
                    data = json.loads(post_data.decode('utf-8'))
                    
                    # å†™å…¥ metadata.json
                    with open(METADATA_PATH, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    print(f"ğŸ’¾ æ•°æ®å·²è‡ªåŠ¨ä¿å­˜åˆ° {METADATA_PATH}")
                    
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.send_header('Access-Control-Allow-Origin', '*')
                    self.end_headers()
                    self.wfile.write(json.dumps({'success': True}).encode('utf-8'))
                except Exception as e:
                    self.send_error(500, str(e))
                return
            
            self.send_error(404, "Not Found")
        
        def do_OPTIONS(self):
            # å¤„ç† CORS é¢„æ£€è¯·æ±‚
            self.send_response(200)
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type')
            self.end_headers()
    
    def run_server():
        # å…è®¸åœ°å€é‡ç”¨ï¼Œé¿å…é‡å¯é¢‘ç¹æ—¶æŠ¥é”™
        HTTPServer.allow_reuse_address = True
        server = HTTPServer(("", SERVER_PORT), ProxyHandler)
        server.serve_forever()
    
    # åå°å¯åŠ¨æœåŠ¡å™¨
    server_thread = Thread(target=run_server, daemon=True)
    server_thread.start()
    
    url = f"http://localhost:{SERVER_PORT}/frontend/index.html"
    print(f"\nğŸŒ æœåŠ¡å™¨å·²å¯åŠ¨: {url}")
    print("   æŒ‰ Ctrl+C é€€å‡º\n")
    
    # æ‰“å¼€æµè§ˆå™¨
    webbrowser.open(url)
    
    # ä¿æŒè¿è¡Œ
    import time
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å†è§!")


async def main():
    """ä¸»æµç¨‹"""
    print("\n" + "="*50)
    print("ğŸ¬ æŠ–éŸ³æ”¶è—æµ·æŠ¥å¢™ - å…¨è‡ªåŠ¨é‡‡é›†")
    print("="*50)
    
    # 1. æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    # 2. ç™»å½•å¹¶æŠ“å–æ”¶è—å¤¹æ•°æ®
    videos = await login_and_scrape_favorites()
    if not videos:
        print("âŒ æœªè·å–åˆ°æ”¶è—æ•°æ®")
        return
    
    # 3. æå–å°é¢ä¿¡æ¯ï¼ˆé¡µé¢æŠ“å–çš„æ•°æ®å·²ç»æ˜¯ç®€åŒ–æ ¼å¼ï¼‰
    # å¦‚æœæ•°æ®å·²ç»æœ‰ cover_urlï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™å°è¯•æå–
    if videos and 'cover_url' in videos[0]:
        covers = videos  # é¡µé¢æŠ“å–çš„æ•°æ®å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
    else:
        covers = extract_cover_data(videos)
    print(f"ğŸ“Š æå–åˆ° {len(covers)} ä¸ªæœ‰æ•ˆå°é¢")
    
    # 5. ä¸‹è½½å°é¢
    covers = await download_covers(covers)
    
    # 6. ä¿å­˜å…ƒæ•°æ®
    save_metadata(covers)
    
    # 7. å¯åŠ¨æœåŠ¡å™¨å¹¶æ‰“å¼€æµè§ˆå™¨
    print("\n" + "="*50)
    print("ğŸ‰ é‡‡é›†å®Œæˆï¼æ­£åœ¨æ‰“å¼€æµ·æŠ¥å¢™...")
    print("="*50)
    
    start_server_and_open_browser()



def monitor_clipboard():
    """åå°ç›‘æ§å‰ªè´´æ¿ï¼Œå‘ç°ç›®æ ‡é“¾æ¥è‡ªåŠ¨å”¤é†’çª—å£"""
    import pyperclip
    import pygetwindow as gw
    
    print("ğŸ“‹ å‰ªè´´æ¿ç›‘æ§å·²å¯åŠ¨...")
    last_text = ""
    
    # æ­£åˆ™è§„åˆ™
    RULES = [
        r"^https?://s\.myhkw\.cn/api\.php\?.*$",  # Shadow Moon API
        r"^https?://.+\.(mp3|m4a|ogg|wav|aac)(\?.*)?$"  # Direct Audio
    ]
    
    while True:
        try:
            text = pyperclip.paste().strip()
            if text and text != last_text:
                last_text = text
                
                # Check match
                is_match = False
                for rule in RULES:
                    if re.match(rule, text, re.I):
                        is_match = True
                        break
                
                if is_match:
                    print(f"\n[Clipboard] Captured: {text[:50]}...")
                    # Find window
                    target_title = "æŠ–éŸ³æ”¶è—æµ·æŠ¥å¢™"
                    windows = gw.getWindowsWithTitle(target_title)
                    
                    if windows:
                        win = windows[0]
                        if not win.isActive:
                            print(f"[Focus] Bringing '{target_title}' to front...")
                            try:
                                if win.isMinimized:
                                    win.restore()
                                win.activate()
                            except Exception as e:
                                print(f"[Focus Error] {e}")
                    else:
                        print(f"[Focus Warning] Window '{target_title}' not found")
                        
            time.sleep(1)
        except Exception as e:
            print(f"[Clipboard Error] {e}")
            time.sleep(2)


if __name__ == "__main__":
    # Start clipboard monitor in background if dependencies are met
    if check_dependencies():
        try:
            monitor_thread = Thread(target=monitor_clipboard, daemon=True)
            monitor_thread.start()
        except Exception as e:
            print(f"âŒ æ— æ³•å¯åŠ¨å‰ªè´´æ¿ç›‘æ§: {e}")

    if len(sys.argv) > 1 and sys.argv[1] == "server":
        print("ğŸš€ ä»…å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼")
        start_server_and_open_browser()
    else:
        asyncio.run(main())
