"""
å°é¢å›¾ç‰‡æ‰¹é‡ä¸‹è½½è„šæœ¬
"""
import os
import json
import asyncio
import aiohttp
import aiofiles
from pathlib import Path
from urllib.parse import urlparse

from config import COVERS_DIR, OUTPUT_DIR


async def download_cover(session: aiohttp.ClientSession, url: str, save_path: Path) -> bool:
    """
    ä¸‹è½½å•å¼ å°é¢å›¾ç‰‡
    
    Args:
        session: aiohttpä¼šè¯
        url: å›¾ç‰‡URL
        save_path: ä¿å­˜è·¯å¾„
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
            if resp.status == 200:
                content = await resp.read()
                async with aiofiles.open(save_path, "wb") as f:
                    await f.write(content)
                return True
            else:
                print(f"âš ï¸ ä¸‹è½½å¤±è´¥ [{resp.status}]: {url}")
                return False
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¼‚å¸¸: {e}")
        return False


async def batch_download_covers(metadata_path: str = None, concurrency: int = 10):
    """
    æ‰¹é‡ä¸‹è½½å°é¢å›¾ç‰‡
    
    Args:
        metadata_path: å…ƒæ•°æ®JSONè·¯å¾„
        concurrency: å¹¶å‘ä¸‹è½½æ•°
    """
    metadata_path = metadata_path or str(Path(OUTPUT_DIR) / "metadata.json")
    
    if not os.path.exists(metadata_path):
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
        print("   è¯·å…ˆè¿è¡Œ collect.py é‡‡é›†æ•°æ®")
        return
    
    with open(metadata_path, "r", encoding="utf-8") as f:
        covers = json.load(f)
    
    print(f"ğŸ“· å‡†å¤‡ä¸‹è½½ {len(covers)} å¼ å°é¢...")
    
    covers_dir = Path(COVERS_DIR)
    covers_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(concurrency)
    
    async def download_with_semaphore(session, cover):
        async with semaphore:
            url = cover.get("cover_url", "")
            if not url:
                return None
            
            # ç”Ÿæˆæ–‡ä»¶å
            video_id = cover.get("id", "unknown")
            ext = urlparse(url).path.split(".")[-1] or "jpg"
            if len(ext) > 5:  # é˜²æ­¢URLæ²¡æœ‰æ‰©å±•å
                ext = "jpg"
            save_path = covers_dir / f"{video_id}.{ext}"
            
            # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
            if save_path.exists():
                return cover
            
            success = await download_cover(session, url, save_path)
            if success:
                cover["local_cover"] = str(save_path.relative_to(Path(OUTPUT_DIR).parent))
                return cover
            return None
    
    # å¹¶å‘ä¸‹è½½
    connector = aiohttp.TCPConnector(limit=concurrency)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [download_with_semaphore(session, cover) for cover in covers]
        results = await asyncio.gather(*tasks)
    
    # æ›´æ–°å…ƒæ•°æ®ï¼ˆæ·»åŠ æœ¬åœ°è·¯å¾„ï¼‰
    updated_covers = [r for r in results if r is not None]
    
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(updated_covers, f, ensure_ascii=False, indent=2)
    
    success_count = len([r for r in results if r is not None])
    print(f"âœ… ä¸‹è½½å®Œæˆ: {success_count}/{len(covers)}")
    print(f"ğŸ“ å°é¢ä¿å­˜äº: {covers_dir}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ‰¹é‡ä¸‹è½½å°é¢")
    parser.add_argument("--input", type=str, help="å…ƒæ•°æ®JSONè·¯å¾„")
    parser.add_argument("--concurrency", type=int, default=10, help="å¹¶å‘æ•°")
    args = parser.parse_args()
    
    asyncio.run(batch_download_covers(args.input, args.concurrency))
