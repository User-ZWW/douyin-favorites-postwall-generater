"""
æŠ–éŸ³æ”¶è—å¤¹é‡‡é›†è„šæœ¬
Derived from: erma0/douyin CLIæ¨¡å¼
"""
import sys
import os
import json
import asyncio
from pathlib import Path

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_DIR = SCRIPT_DIR.parent

# æ·»åŠ å¤–éƒ¨ä¾èµ–è·¯å¾„
sys.path.insert(0, str(PROJECT_DIR / "external/douyin"))

from config import COOKIE, MAX_ITEMS

# ä½¿ç”¨ç»å¯¹è·¯å¾„è§£æè¾“å‡ºç›®å½•
OUTPUT_DIR = PROJECT_DIR / "data"


async def collect_favorites(cookie: str = None) -> list:
    """
    é‡‡é›†å½“å‰ç”¨æˆ·çš„æ”¶è—å¤¹
    
    Args:
        cookie: æŠ–éŸ³Cookieï¼Œå¦‚æœä¸ºç©ºåˆ™ä»configè¯»å–
        
    Returns:
        è§†é¢‘æ•°æ®åˆ—è¡¨
    """
    cookie = cookie or COOKIE
    if not cookie:
        print("âŒ è¯·å…ˆåœ¨ config.py ä¸­é…ç½® COOKIE")
        print("   è·å–æ–¹æ³•ï¼šç™»å½•æŠ–éŸ³ç½‘é¡µç‰ˆ -> F12 -> Application -> Cookies")
        return []
    
    # è°ƒç”¨erma0/douyinçš„CLIæ¥å£
    # æ³¨æ„ï¼šéœ€è¦æ ¹æ®å®é™…APIè°ƒæ•´
    try:
        from backend.api import DouyinAPI
        
        api = DouyinAPI(cookie=cookie)
        
        # è·å–å½“å‰ç”¨æˆ·æ”¶è—
        print("ğŸ”„ æ­£åœ¨è·å–æ”¶è—å¤¹æ•°æ®...")
        favorites = await api.get_user_favorites(max_count=MAX_ITEMS)
        
        # ä¿å­˜åŸå§‹æ•°æ®
        output_path = Path(OUTPUT_DIR) / "raw_favorites.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(favorites, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é‡‡é›†å®Œæˆï¼Œå…± {len(favorites)} ä¸ªè§†é¢‘")
        print(f"ğŸ“ åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
        
        return favorites
        
    except ImportError as e:
        print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿å·²æ­£ç¡®å…‹éš† erma0/douyin åˆ° external/douyin")
        return []
    except Exception as e:
        print(f"âŒ é‡‡é›†å¤±è´¥: {e}")
        return []


def extract_cover_data(videos: list) -> list:
    """
    ä»è§†é¢‘æ•°æ®ä¸­æå–å°é¢ä¿¡æ¯
    
    Args:
        videos: åŸå§‹è§†é¢‘æ•°æ®åˆ—è¡¨
        
    Returns:
        å°é¢å…ƒæ•°æ®åˆ—è¡¨
    """
    covers = []
    
    for video in videos:
        try:
            cover_info = {
                "id": video.get("aweme_id", ""),
                "title": video.get("desc", "æ— æ ‡é¢˜"),
                "author": video.get("author", {}).get("nickname", "æœªçŸ¥"),
                "author_id": video.get("author", {}).get("sec_uid", ""),
                "cover_url": video.get("video", {}).get("cover", {}).get("url_list", [""])[0],
                "dynamic_cover": video.get("video", {}).get("dynamic_cover", {}).get("url_list", [""])[0],
                "video_url": f"https://www.douyin.com/video/{video.get('aweme_id', '')}",
                "create_time": video.get("create_time", 0),
            }
            
            if cover_info["cover_url"]:
                covers.append(cover_info)
                
        except Exception as e:
            print(f"âš ï¸ è§£æè§†é¢‘æ•°æ®å¤±è´¥: {e}")
            continue
    
    return covers


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æŠ–éŸ³æ”¶è—å¤¹é‡‡é›†")
    parser.add_argument("--cookie", type=str, help="æŠ–éŸ³Cookie")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼")
    args = parser.parse_args()
    
    if args.test:
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = [
            {
                "aweme_id": f"test_{i}",
                "desc": f"æµ‹è¯•è§†é¢‘æ ‡é¢˜ {i}",
                "author": {"nickname": f"ä½œè€…{i}", "sec_uid": f"uid_{i}"},
                "video": {
                    "cover": {"url_list": [f"https://picsum.photos/seed/{i}/300/400"]},
                    "dynamic_cover": {"url_list": [""]}
                },
                "create_time": 1700000000 + i * 86400
            }
            for i in range(50)
        ]
        covers = extract_cover_data(test_data)
        
        output_path = Path(OUTPUT_DIR) / "metadata.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(covers, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æµ‹è¯•æ•°æ®å·²ç”Ÿæˆ: {output_path}")
    else:
        videos = asyncio.run(collect_favorites(args.cookie))
        if videos:
            covers = extract_cover_data(videos)
            
            output_path = Path(OUTPUT_DIR) / "metadata.json"
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(covers, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“ å…ƒæ•°æ®å·²ä¿å­˜è‡³: {output_path}")
